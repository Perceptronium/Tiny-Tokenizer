{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b288b9b6",
      "metadata": {},
      "source": [
        "Notebook by Can Pouliquen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naïve BPE (don't use this, it's excruciatingly slow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import regex as re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe52bd6",
      "metadata": {},
      "source": [
        "##### Take a random toy corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = \" low low low low low lower lower widest widest widest newest newest newest newest newest newest\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aedcb93",
      "metadata": {},
      "source": [
        "##### Initialize a vocab `dict[int,bytes]` with the 256 possible byte values and tokenize the corpus with those"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b'e', b's', b't', b' ', b'n', b'e', b'w', b'e', b's', b't']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {i: bytes([i]) for i in range(256)}\n",
        "tokens = [bytes([b]) for b in corpus.encode(\"utf-8\")]\n",
        "tokens[-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d13601",
      "metadata": {},
      "source": [
        "##### Initialize a pair counter `dict[[bytes, bytes],int]`, iterate over all adjacent pairs across the whole corpus and update the counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({(b'e', b's'): 9,\n",
              "         (b's', b't'): 9,\n",
              "         (b'w', b'e'): 8,\n",
              "         (b't', b' '): 8,\n",
              "         (b' ', b'l'): 7,\n",
              "         (b'l', b'o'): 7,\n",
              "         (b'o', b'w'): 7,\n",
              "         (b' ', b'n'): 6,\n",
              "         (b'n', b'e'): 6,\n",
              "         (b'e', b'w'): 6,\n",
              "         (b'w', b' '): 5,\n",
              "         (b' ', b'w'): 3,\n",
              "         (b'w', b'i'): 3,\n",
              "         (b'i', b'd'): 3,\n",
              "         (b'd', b'e'): 3,\n",
              "         (b'e', b'r'): 2,\n",
              "         (b'r', b' '): 2})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pair_counter = Counter()\n",
        "for pair in zip(tokens[:-1], tokens[1:]):\n",
        "    pair_counter[pair] += 1\n",
        "pair_counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97296eb",
      "metadata": {},
      "source": [
        "##### Pick the most frequent pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c5b9e752",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(b's', b't')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "most_frequent_pair = max(pair_counter, key=lambda p: (pair_counter[p], p))\n",
        "most_frequent_pair"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4a290f",
      "metadata": {},
      "source": [
        "##### Merge that pair into a new bytes token and add it to the vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d0d93c5d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'st'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_token = most_frequent_pair[0] + most_frequent_pair[1]\n",
        "\n",
        "vocab[len(vocab)] = new_token\n",
        "\n",
        "vocab[len(vocab)-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd93aa2c",
      "metadata": {},
      "source": [
        "##### Scan the entire corpus again to find occurrences of that pair and replace them with the merged token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged = []\n",
        "i = 0\n",
        "while i < len(tokens):\n",
        "    if i + 1 < len(tokens) and (tokens[i], tokens[i + 1]) == most_frequent_pair:\n",
        "        merged.append(new_token)\n",
        "        i += 2\n",
        "    else:\n",
        "        merged.append(tokens[i])\n",
        "        i += 1\n",
        "tokens = merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9f04b3bf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b' ', b'n', b'e', b'w', b'e', b'st', b' ', b'n', b'e', b'w', b'e', b'st']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[-12:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d174b0ff",
      "metadata": {},
      "source": [
        "#### Repeat this cycle until a desired vocab size is reached\n",
        "#### This is extremely slow because for each new merge, it requires iterating twice on the whole corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f4b4cc6",
      "metadata": {},
      "source": [
        "# Improving the naïve implem in 3 main ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709ee9fc",
      "metadata": {},
      "source": [
        "##### 1- Use pre-tokenization (needs some guarding against special tokens, handled in Tiny-Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7fa27edf",
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = \" low low low low low lower lower widest widest widest newest newest newest newest newest newest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "31aa0604",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' low',\n",
              " ' low',\n",
              " ' low',\n",
              " ' low',\n",
              " ' low',\n",
              " ' lower',\n",
              " ' lower',\n",
              " ' widest',\n",
              " ' widest',\n",
              " ' widest',\n",
              " ' newest',\n",
              " ' newest',\n",
              " ' newest',\n",
              " ' newest',\n",
              " ' newest',\n",
              " ' newest']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokens_splitter = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "words = re.findall(pre_tokens_splitter, corpus)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b12c08a7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({' newest': 6, ' low': 5, ' widest': 3, ' lower': 2})"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokens_ctr = Counter()\n",
        "pre_tokens_ctr.update(words)\n",
        "pre_tokens_ctr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7aabeab",
      "metadata": {},
      "source": [
        "##### It will be convenient to represent this as a `dict[tuple[bytes], int]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "2baf9d71",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(b' ', b'l', b'o', b'w'): 5,\n",
              " (b' ', b'l', b'o', b'w', b'e', b'r'): 2,\n",
              " (b' ', b'w', b'i', b'd', b'e', b's', b't'): 3,\n",
              " (b' ', b'n', b'e', b'w', b'e', b's', b't'): 6}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokens = {tuple(bytes([b]) for b in key.encode('utf-8')): pre_tokens_ctr[key] for key in pre_tokens_ctr.keys()}\n",
        "pre_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075e540b",
      "metadata": {},
      "source": [
        "#### We can now count adjacent pairs by adding the frequency of the pre-token instead of incrementally !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ce60951b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({(b'e', b's'): 9,\n",
              "         (b's', b't'): 9,\n",
              "         (b'w', b'e'): 8,\n",
              "         (b' ', b'l'): 7,\n",
              "         (b'l', b'o'): 7,\n",
              "         (b'o', b'w'): 7,\n",
              "         (b' ', b'n'): 6,\n",
              "         (b'n', b'e'): 6,\n",
              "         (b'e', b'w'): 6,\n",
              "         (b' ', b'w'): 3,\n",
              "         (b'w', b'i'): 3,\n",
              "         (b'i', b'd'): 3,\n",
              "         (b'd', b'e'): 3,\n",
              "         (b'e', b'r'): 2})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pair_counter = Counter()\n",
        "for pre_token in pre_tokens:\n",
        "    for pair in zip(pre_token[:-1], pre_token[1:]):\n",
        "        pair_counter[pair] += pre_tokens[pre_token]\n",
        "pair_counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b61459",
      "metadata": {},
      "source": [
        "##### For each merge, we still have to go twice through the corpus, but we have effectively \"reduced its size\" (we go twice through the pre-tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508492d5",
      "metadata": {},
      "source": [
        "##### 2- Parallelize pre-tokenization across CPU cores (see the implem in Tiny-Tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc06a47",
      "metadata": {},
      "source": [
        "##### 3- Caching : cache the pair-counter and in which pre-token each pair occurs \n",
        "(slightly more involved, so easier to follow orally, see the implem in Tiny-Tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24db7c3b",
      "metadata": {},
      "source": [
        "# Once the vocab is built, you can encode/decode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9ebd595f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "from models.tokenizer import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2c2314dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[72, 101, 294, 111, 1420, 348, 104, 306, 44, 338, 366, 107, 101, 2177, 116, 115, 46]\n"
          ]
        }
      ],
      "source": [
        "path = Path(\"./results/TinyStoriesV2-GPT4-train_bpe_vocab.pkl\")\n",
        "with path.open(\"rb\") as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "path = Path(\"./results/TinyStoriesV2-GPT4-train_bpe_merges.pkl\")\n",
        "with path.open(\"rb\") as f:\n",
        "    merges = pickle.load(f)\n",
        "\n",
        "tokenizer = Tokenizer(vocab=vocab, merges=merges, special_tokens=\"<|endoftext|>\" or None)\n",
        "\n",
        "prompt = \"Hello Ockham, I like cats.\"\n",
        "token_ids = tokenizer.encode(prompt)\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6551d82c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello Ockham, I like cats.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3617b8c8",
      "metadata": {},
      "source": [
        "##### Encoding is different depending on the dataset you train on and obviously the vocab size ! Let's look at how \"Ockham\" is encoded when training on TinyStories (vocab = 10k tokens) vs OpenWebText (vocab = 32k tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "42c1a029",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[79, 348, 104, 306]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# With TinyStories vocab\n",
        "tokenizer.encode(\"Ockham\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b89388be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[79, 726, 3147]\n"
          ]
        }
      ],
      "source": [
        "path = Path(\"./results_old/open_web_text_bpe_vocab.pkl\")\n",
        "with path.open(\"rb\") as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "path = Path(\"./results_old/open_web_text_bpe_merges.pkl\")\n",
        "with path.open(\"rb\") as f:\n",
        "    merges = pickle.load(f)\n",
        "\n",
        "tokenizer = Tokenizer(vocab=vocab, merges=merges, special_tokens=\"<|endoftext|>\" or None)\n",
        "\n",
        "prompt = \"Ockham\"\n",
        "token_ids = tokenizer.encode(prompt)\n",
        "print(token_ids)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.13 ('these-can')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3816b396561f4adb91ebd46b1899e03d47819dc361110f762bf8f14ad4c11645"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
